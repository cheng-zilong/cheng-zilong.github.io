<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Orandragon&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Orandragon&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Orandragon&#39;s Blog">
  <link rel="canonical" href="http://yoursite.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Orandragon's Blog</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Orandragon's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            
  <div id="posts" class="posts-expand">
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/" class="post-title-link" itemprop="url">4. Gradient Methods (1)</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-06 13:19:39" itemprop="dateCreated datePublished" datetime="2019-09-06T13:19:39+08:00">2019-09-06</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-26 10:04:43" itemprop="dateModified" datetime="2019-09-26T10:04:43+08:00">2019-09-26</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/" class="post-meta-item leancloud_visitors" data-flag-title="4. Gradient Methods (1)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/09/06/Nonlinear Optimization/4.1 Gradient methods/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="thanks-for-the-module-ma5268-nus-and-prof.-toh">Thanks for the module MA5268 NUS and Prof. Toh</h1>
<h1 id="gradient-methods">Gradient methods</h1>
<h2 id="gradient-methods-for-unconstrained-optimization-problem">4.1 Gradient methods for unconstrained optimization problem</h2>
<p>Consider the multidimensional unconstrained minimization problem <span class="math display">\[
\begin{array}{rCl}
\text{minimize} &amp; f(x)\\
\text{subject to}&amp; x\in S.
\end{array}
\]</span> In the situation where <span class="math inline">\(\nabla f(x)=0\)</span> cannot be solved analytically, we look for an approximate solution via iterative methods.</p>
<p>General framework of an optimization algorithm.</p>
<hr>
<p>For <span class="math inline">\(k=0,1,...,\)</span> if <span class="math inline">\(x^{(k)}\)</span> is optimal stop else determine an improved estimate of the solution <span class="math inline">\(x^{(k+1)}=x^{(k)}+\alpha_{k}\times p^{(k)}\)</span> end end</p>
<hr>
<p>Here <span class="math inline">\(p^{(k)}\)</span> is a <strong>search direction</strong> that we hope points towards the solution, or that improves our solution in some sense. The scalar <span class="math inline">\(\alpha_k\)</span> is a step length that determines the point <span class="math inline">\(x^{(k+1)}\)</span>. Once the search direction has been solved, the step length can be solved as some auxiliary one-dimensional problems.</p>
<p><strong>Descent property</strong> At a given point <span class="math inline">\(x^*\)</span>, let <span class="math inline">\(\hat d=-\frac{\nabla f(x^*)}{||\nabla f(x^*)||}\)</span>. The value of <span class="math inline">\(f\)</span> decreases most rapidly along the unit direction <span class="math inline">\(\hat d\)</span> and the rate of change of <span class="math inline">\(f\)</span> at <span class="math inline">\(x^*\)</span> along the direction <span class="math inline">\(\hat d\)</span> is <span class="math inline">\(-||\nabla f(x^*)||\)</span>, i.e., as <span class="math inline">\(x\)</span> moves along <span class="math inline">\(\hat d\)</span> from <span class="math inline">\(x^*\)</span> by a small distance <span class="math inline">\(\delta\)</span>, the value <span class="math inline">\(f(x)\)</span> is changed by the amount <span class="math inline">\(-||\nabla f(x^*)||\delta\)</span>.</p>
<p>In general, a direction <span class="math inline">\(d\)</span> such that <span class="math inline">\(\langle \nabla f(x^*),d\rangle &lt;0\)</span> is called a descent direction.</p>
<p>The direction <span class="math inline">\(-\nabla f(x^*)\)</span> is known as the steepest descent direction since it gives the fastest rate of decrease in <span class="math inline">\(f(x)\)</span> among all directions.</p>
<p><strong>Steepest descent method with exact line search</strong></p>
<hr>
<p><strong>[step 0]</strong> Select an initial point <span class="math inline">\(x^{(0)}\)</span>, and <span class="math inline">\(\epsilon&gt;0\)</span> <strong>[step k]</strong> For <span class="math inline">\(k=0,1,...,\)</span></p>
<ol type="1">
<li><p>evaluate <span class="math inline">\(d^{(k)} = -\nabla f(x^{(k)})\)</span></p></li>
<li><p>if <span class="math inline">\(||d^{(k)}||&lt;\epsilon\)</span>, stop and <span class="math inline">\(x^{(k)}\)</span> is an approximate solution</p>
<p>else</p>
<ol type="1">
<li><p>find the value <span class="math inline">\(t_k\)</span> that minimizes the one-dimensional function <span class="math display">\[
g(t)=f(x^{(k)}+td^{(k)})
\]</span></p></li>
<li><p>set <span class="math inline">\(x^{(k+1)}=x^{(k)}+t_kd^{(k)}\)</span>.</p></li>
</ol></li>
</ol>
<p>end</p>
<hr>
<p><strong>Remark</strong></p>
<p>The most difficult part of the steepest descent method usually is to find the <span class="math inline">\(t_k\)</span> that minimizes <span class="math inline">\(f\)</span> along the gradient direction.</p>
<p><strong>Theorem 4.1</strong> The steepest descent method with exact linesearch moves in <strong>perpendicular</strong> steps.</p>
<p><strong>Remarks</strong></p>
<ol type="1">
<li><p>Monotonic decreasing property</p>
<p>If <span class="math inline">\(x^{(k)}\)</span> is a steepest descent sequence for a function <span class="math inline">\(f(x)\)</span>, and if <span class="math inline">\(\nabla f(x^{(k)})\neq 0\)</span> for some <span class="math inline">\(k\)</span>, then <span class="math inline">\(f(x^{(k+1)})&lt;f(x^{(k)})\)</span>.</p></li>
<li><p>Convergence of a steepest descent method</p>
<p>Suppose <span class="math inline">\(f(x)\)</span> is a coercive function with continuous first derivatives on <span class="math inline">\(\mathbb R^n\)</span>. Let <span class="math inline">\(x^{(0)}\in \mathbb R^n\)</span>. Suppose <span class="math inline">\(\{x^{(k)}\}\)</span> is the steepest descent sequence for <span class="math inline">\(f(x)\)</span> with initial point <span class="math inline">\(x^{(0)}\)</span>. Then some subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> converges. The limit of any convergent subsequence of <span class="math inline">\(\{x^{(k)}\}\)</span> is a critical point of <span class="math inline">\(f(x)\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-of-the-steepest-descent-method-for-an-unconstrained-convex-quadratic-minimization-problem">4.1.1 Convergence rate of the steepest descent method for an unconstrained convex quadratic minimization problem</h3>
<p>Here we consider <span class="math display">\[
\text{minimize}_{x\in\mathbb R^n}\; q(x)=\frac{1}{2}x^TQx,
\]</span> where <span class="math inline">\(Q\)</span> is symmetric positive definite.</p>
<p><strong>Proposition</strong></p>
<p>For a symmetric positive definite <span class="math inline">\(Q\)</span>, suppose that <span class="math inline">\(\{x^{(k)}\}\)</span> is a sequence obtained from the steepest descent method with exact line search applied to the function <span class="math inline">\(q(x)\)</span>. Then</p>
<ol type="1">
<li><p>Let <span class="math inline">\(d^{k}=\nabla q(x^k)=Qx^k\)</span>, <span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}=1-\frac{\langle d^k,d^k\rangle^2}{\langle d^k,Qd^k\rangle\langle d^k,Q^{-1}d^k\rangle}
\]</span></p></li>
<li><p><span class="math display">\[
\frac{q(x^{k+1})}{q(x^{k})}\le \left[\frac{\kappa (Q)-1}{\kappa (Q)+1}\right]^2=\rho(Q)
\]</span></p>
<p>where <span class="math inline">\(\kappa (Q) = \lambda_n/\lambda_1\)</span>, and <span class="math inline">\(\lambda_n\)</span> and <span class="math inline">\(\lambda_1\)</span> are the largest and smallest eigenvalues of <span class="math inline">\(Q\)</span>, respectively. The number <span class="math inline">\(\kappa(Q)\)</span> is called the condition number of <span class="math inline">\(Q\)</span>. When <span class="math inline">\(\kappa(Q)\ge 1\)</span> is small, say less than <span class="math inline">\(10^3\)</span>, <span class="math inline">\(Q\)</span> is said to be well-conditioned.</p></li>
</ol>
<p>Proof is intuitive and skipped.</p>
<p><strong>Remark</strong></p>
<ol type="1">
<li><p>From proposition, we see that the convergence rate <span class="math inline">\(\rho (Q)\)</span> of the steepest descent method depends on <span class="math inline">\(\kappa(Q)\)</span>. When <span class="math inline">\(\kappa(Q)\)</span> is large, the convergence rate <span class="math display">\[
\rho(Q)\approx 1-\frac{4}{\kappa(Q)}.
\]</span></p></li>
<li><p>The number of iterations needed to reduce the relative error <span class="math inline">\(q(x_k)/q(x_0)\)</span> to smaller than <span class="math inline">\(\epsilon\)</span> is given by <span class="math display">\[
k=\left[\frac{\log \epsilon }{\log \rho(Q)}\right]+1
\]</span> where <span class="math inline">\([a]\)</span> denotes the largest integer less than or equal to <span class="math inline">\(a\)</span>.</p></li>
</ol>
<h3 id="convergence-rate-for-the-steepest-descent-method-for-strongly-convex-function">4.1.2 Convergence rate for the steepest descent method for strongly convex function</h3>
<p>Let <span class="math inline">\(S\subset \mathbb R^n\)</span> be a convex set and <span class="math inline">\(f:S\rightarrow \mathbb R\)</span> a convex function. We assume that <span class="math inline">\(f\)</span> is strongly convex with parameter <span class="math inline">\(m\)</span> and has <span class="math inline">\(M\)</span>-Lipschitz continuous gradient on <span class="math inline">\(S\)</span>. Then its Hessian satisfies the following property, <span class="math display">\[
mI\preceq H_f(x)\preceq MI\;\forall x\in S.
\]</span> <strong>Lemma</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer. Then <span class="math display">\[
f(x)-\frac{1}{2m}||\nabla f(x)||^2\le f(x^*)\le f(x)-\frac{1}{2M}||\nabla f(x)||^2 \; \forall x\in S.
\]</span> <strong>Theorem</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be strongly convex with parameter <span class="math inline">\(m\)</span> and its gradient is <span class="math inline">\(M\)</span>-Lipschitz. Let <span class="math inline">\(x^*\)</span> be the unique of <span class="math inline">\(f\)</span> over <span class="math inline">\(\mathbb R^n\)</span>. Define <span class="math inline">\(E_k = f(x^k)-f(x^*)\)</span>, where <span class="math inline">\(\{x^k\}\)</span> is generated by the steepest descent method with exact linesearch. Then, <span class="math display">\[
E_{k+1}\le E_k-\frac{1}{2M}||\nabla f(x^k)||^2\\
E_{k+1}\le E_k\left(1-\frac{m}{M}\right).
\]</span> Proof.</p>
<p>By using the property of <span class="math inline">\(M\)</span>-Lipschitz, we have <span class="math display">\[
f(y)\le f(x)+\langle \nabla f(x),y-x \rangle+\frac{M}{2}||y-x||^2\;\forall x,y\in\mathbb R^n.
\]</span></p>
<p>If we choose <span class="math inline">\(y=x^k-td^k\)</span>, <span class="math inline">\(d^k=\nabla f(x^k)\)</span> and <span class="math inline">\(x=x^k\)</span>, we have <span class="math display">\[
f(x^k-td^k)\le f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> We define the function <span class="math display">\[
f(t)=f(x^k)-t\langle \nabla f(x^k),d^k \rangle+\frac{Mt^2}{2}||d^k||^2.
\]</span> Then we have <span class="math display">\[
f_{\min}=f(\frac{1}{M})=f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> Therefore, we have <span class="math display">\[
f(x^{k+1})\le f(x^k)-\frac{1}{2M}||d^k||^2.
\]</span> The first inequality can be proved.</p>
<p>For the second inequality, since we have <span class="math display">\[
E_k\le \frac{1}{2m}||\nabla f(x)||^2.
\]</span> Then we have <span class="math inline">\(-||d^k||^2\le -2mE_k\)</span>. The second inequality can be proved.</p>
<p><strong>Remark</strong></p>
<p>From the theorem, we see that <span class="math display">\[
E(x^{k+1})/E(x^1)\le (1-m/M)^k\le \epsilon,
\]</span> which implies that we need the number of iterations <span class="math inline">\(k\)</span> to satisfy <span class="math display">\[
k\ge \frac{\log \epsilon ^{-1}}{\log\rho^{-1}}\approx \frac{m}{M}\log \epsilon^{-1} \;(\text{ if }m/M\ll 1),
\]</span> where <span class="math inline">\(\rho = 1-m/M\)</span>.</p>
<h2 id="line-search-strategies">4.2 Line search strategies</h2>
<ol type="1">
<li><p>Minimization rule = exact line search. <span class="math display">\[
\alpha_k=\arg\min\{f(x^k+\alpha d^k)\;|\;\alpha \ge 0\}.
\]</span> If the line search interval is limited to <span class="math inline">\(\alpha \in [0,\bar \alpha]\)</span>, it is called limited minimization rule.</p></li>
<li><p>Armijo rule (<strong>backtracking</strong> method).</p>
<p>Let <span class="math inline">\(\sigma\in(0,0.5)\)</span> and <span class="math inline">\(\beta\in (0,1)\)</span>. Start with <span class="math inline">\(\bar\alpha\)</span> and continue with $=,,$ until the following inequality is satisfied, <span class="math display">\[
f(x^k+\alpha d^k)\le f(x^k)+\alpha\sigma\langle \nabla f(x^k),d^k\rangle.
\]</span> Let <span class="math inline">\(r\)</span> be the first integer satisfying the inequality. Set <span class="math inline">\(\alpha_k=\beta^r\bar \alpha\)</span>.</p></li>
<li><p>Non-monotone line search.</p></li>
</ol>
<h2 id="accelerated-proximal-gradient-method-for-convex-programming">4.3 Accelerated proximal gradient method for convex programming</h2>
<p>Consider a smooth convex function <span class="math inline">\(f\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. We are interested in solving <span class="math display">\[
\min \{F(x)=f(x)+g(x)\;|\;x\in \mathbb R^n\},
\]</span> where <span class="math inline">\(g:\mathbb R^n\rightarrow (-\infty,\infty]\)</span> is a proper closed convex function.</p>
<p><strong>Example</strong></p>
<p>In sparse regression problem, <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho ||x||_1\)</span>.</p>
<p>For a given <span class="math inline">\(\bar x\)</span> and <span class="math inline">\(H\succeq 0\)</span>, consider the convex quadratic function, <span class="math display">\[
q(x;\bar x)= f(\bar x)+\langle \nabla f(\bar x),x-\bar x\rangle +\frac{1}{2}\langle x-\bar x,H(x-\bar x)\rangle.
\]</span> At the current point <span class="math inline">\(\bar x\)</span>, proximal gradient and APG methods solve a sub-problem of the form, <span class="math display">\[
\hat x =\arg \min \{g(x)+q(x;\bar x)\;|\;x\in \mathbb R^n\}.
\]</span> <strong>Accelerated proximal gradient (APG) method</strong></p>
<p>Given a positive sequence <span class="math inline">\(\{t_k\}\)</span> such that <span class="math inline">\(t_{k+1}^2-t_{k+1}\le t_k^2\)</span> starting with <span class="math inline">\(t_0=1\)</span>, <span class="math inline">\(t_1=1\)</span>. Given <span class="math inline">\(x^0\)</span>. For <span class="math inline">\(k=0,1,\dots\)</span>, do the following iterations.</p>
<p><strong>[Step 1]</strong> Set <span class="math inline">\(\beta_k=(t_k-1)/t_{k+1}\)</span> and <span class="math inline">\(\bar x^k=x^k+\beta(x^k-x^{k-1})\)</span>.</p>
<p><strong>[Step 2]</strong> Compute <span class="math display">\[
x^{k+1}=\arg \min \{g(x)+q(x;\bar x^k)\;|\;x\in \mathbb R^n\}.
\]</span> When <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>, then <span class="math inline">\(\bar x^k=x^k\)</span> for all <span class="math inline">\(k\)</span>, and the method is the standard <strong>proximal gradient method</strong>.</p>
<p><strong>Lemma</strong></p>
<p>Assume that <span class="math inline">\(f(\hat x)\le q(\hat x;\bar x)\)</span>. (This assumption can be satisfied by choosing <span class="math inline">\(H\)</span>) Then we have the following <strong>decent property</strong>, <span class="math display">\[
F(x)+\frac{1}{2}||x-\bar x||_H^2\ge F(\hat x)+\frac{1}{2}||x-\hat x||_H^2\;\forall x\in \mathbb R^n.
\]</span> Proof.</p>
<p>We have <span class="math display">\[
\begin{array}{rCl}
F(x)-F(\hat x)&amp;=&amp;F(x)-f(\hat x)-g(\hat x)\\
&amp;\ge&amp; F(x)-q(\hat x;\bar x)-g(\hat x)\\
&amp;=&amp; g(x)-g(\hat x)+f(x)-f(\bar x)-\langle \nabla f(\bar x),\hat x-\bar x\rangle-\frac{1}{2}||\hat x-\bar x||_H^2.
\end{array}
\]</span> By convexity of <span class="math inline">\(g\)</span> and <span class="math inline">\(f\)</span>, we have <span class="math display">\[
g(x)-g(\hat x)\ge \langle \lambda,x-\hat x\rangle\quad \lambda \in \partial g(\hat x)\\
f(x)-f(\bar x)\ge \langle \nabla f(\bar x),x-\bar x\rang.
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle \lambda+\nabla f(\bar x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2.
\]</span> From the optimality condition, we have <span class="math display">\[
0\in \partial g(\hat x)+\nabla f(\bar x)+H(\hat x-\bar x)\\
\implies\\
\lambda +\nabla f(\bar x)= H(\bar x-\hat x).
\]</span> Then we have <span class="math display">\[
F(x)-F(\hat x)\ge \langle H(\bar x-\hat x),x-\hat x\rangle -\frac{1}{2}||\hat x-\bar x||_H^2\\
=\frac{1}{2}||x-\hat x||_H^2-\frac{1}{2}||x-\bar x||_H^2.
\]</span> Q.E.D.</p>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(x^*\)</span> be a minimizer of <span class="math inline">\(F(\cdot)\)</span>. Define <span class="math inline">\(E(\cdot)=F(\cdot)-F(x^*)\ge 0\)</span>. Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
E(x^{k+1})\le \frac{1}{2t_{k+1}^2}||x^*-x^0||_H^2.
\]</span> In practice, the sequence <span class="math inline">\(\{t_k\}\)</span> is typically defined recursively as follows, <span class="math display">\[
t_{k+1}=\frac{1+\sqrt{1+4t_k^2}}{2}\implies t_k\ge \frac{k}{2}\quad \forall k\ge 1
\]</span> Hence, <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2t_k^2}||x^*-x^0||_H^2\le \frac{2}{k^2}||x^*-x^0||_H^2.
\]</span> That is, the iteration complexity of APG is <span class="math inline">\(O(1/k^2)\)</span>.</p>
<p><strong>Theorem</strong></p>
<p>Assume that <span class="math inline">\(f(x^{k+1})\le q(x^{k+1};\bar x^k)\)</span> for all <span class="math inline">\(k\)</span>. If <span class="math inline">\(t_k=1\)</span> for all <span class="math inline">\(k\)</span>. Then <span class="math display">\[
kE(x^k)+\frac{1}{2}||x^k-x^*||_H^2\le E(x^1)+\frac{1}{2}||x^1-x^*||_H^2\le \frac{1}{2}||x^0-x^*||_H^2.
\]</span> Hence <span class="math display">\[
0\le F(x^k)-F(x^*)\le \frac{1}{2k}||x^0-x^*||_H^2.
\]</span> That is, the iteration complexity of the proximal gradient method is <span class="math inline">\(O(1/k)\)</span>.</p>
<p><strong>Example</strong></p>
<p>Consider the sparse regression problem, <span class="math display">\[
\min \{\frac{1}{2}||Ax-b||^2+\rho||x||_1\;|\; x\in\mathbb R^n\},
\]</span> where <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>, <span class="math inline">\(b\in \mathbb R^m\)</span> and <span class="math inline">\(\rho\)</span> are given data. Let <span class="math inline">\(f(x)=\frac{1}{2}||Ax-b||^2\)</span> and <span class="math inline">\(g(x)=\rho||x||_1\)</span>. Then <span class="math inline">\(\nabla f(x)=A^T(Ax-b)\)</span> is Lipschitz continuous with modulus <span class="math inline">\(L=\lambda_\max(AA^T)\)</span>. Pick <span class="math inline">\(H=LI_n\)</span>, the APG subproblem is given by <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_{x}\left\{g(x)+\langle \nabla f(\bar x^k),x-\bar x^k\rangle +\frac{L}{2}||x-\bar x^k||^2\;|\; x\in\mathbb R^n\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}(x-\bar x^k+\frac{2}{L}\nabla f(\bar x^k))^2-\frac{2}{L}||\nabla f(\bar x^k)||^2\right\}\\
&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}[x-(\bar x^k-\frac{2}{L}\nabla f(\bar x^k))]^2\right\}
\end{array}
\]</span> Then if we define <span class="math inline">\(y^k=\bar x^k-\frac{2}{L}\nabla f(\bar x^k)\)</span>, we have <span class="math display">\[
\begin{array}{rCl}
x^{k+1}&amp;=&amp;\arg\min_x\left\{ \rho||x||_1+\frac{L}{2}(x-y^k)^2\right\}.
\end{array}
\]</span> To be done.............</p>
<p><strong>Example</strong></p>
<p>Given <span class="math inline">\(G\in \mathbb S^n\)</span>, consider the projection problem onto the closed convex cone <span class="math inline">\(DNN_n^*=S_+^N+\mathcal N^n\)</span>. This problem can be formulated as, <span class="math display">\[
\min\left\{\frac{1}{2}||S+Z-G||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
=\min\left\{\frac{1}{2}||Z-(G-S)||^2\;|\;S\in\mathbb S^n_+,\;Z\in\mathcal N^n\right\}\\
=\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\;|\;S\in\mathbb S^n_+\right\}\\
=\min\left\{\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2+\delta_{\mathbb S_+^n}(S)\right\}\\
\]</span> We define <span class="math inline">\(f(S)=\frac{1}{2}||\Pi_{\mathcal N^n}(S-G)||^2\)</span>. From the optimality condition, we have <span class="math display">\[
0\in\nabla f(\bar S)+\partial \delta_{\mathbb S_+^n}\quad \text{$f(\bar S)$ is differentiable}.\\
-\nabla f(\bar S)\in \delta_{\mathbb S_+^n}(S)\\
\bar S-\nabla f(\bar S)\in (I+\delta_{\mathbb S_+^n})(\bar S)
\]</span> Then we have <span class="math display">\[
\bar S=(I+\delta_{\mathbb S_+^n})^{-1}(\bar S-\nabla f(\bar S))=\Pi_{\mathbb S_+^n}(\bar S-\nabla f(\bar S)).
\]</span> To be done..........</p>
<h2 id="gradient-projection-method">4.4 Gradient projection method</h2>
<p>Let <span class="math inline">\(f:Q\rightarrow \mathbb R\)</span> be a continuously differentiable function (not necessarily convex) defined on a closed convex subset <span class="math inline">\(Q\)</span> of <span class="math inline">\(\mathbb R^n\)</span> with <span class="math inline">\(L\)</span>-Lipschitz continuous gradient. Consider <span class="math display">\[
\min\{f(x)\;|\; x\in Q\}.
\]</span> We say that <span class="math inline">\(\bar x\in Q\)</span> is a stationary point if <span class="math display">\[
\langle \nabla f(\bar x),y-\bar x\rangle \ge 0\quad\forall y\in Q.
\]</span> The gradient projection method generate a sequence of iterates as follows, <span class="math display">\[
x^{k+1}=P_Q(x^k-\alpha \nabla f(x^k)).
\]</span> For each <span class="math inline">\(x\in Q\)</span>, let <span class="math display">\[
x(\alpha )=x-\alpha\nabla f(x),\quad x_Q(\alpha )=P_Q(x(\alpha)).
\]</span></p>
<h2 id="stochastic-gradient-descent-method">4.5 Stochastic gradient descent method</h2>
<p>Suppose <span class="math inline">\(\tilde Z\)</span> is an n-dimensional random variable with mean <span class="math inline">\(\mu \in \mathbb R^n\)</span>. Consider the following problem, <span class="math display">\[
\min_{x\in\mathbb R^n}h(x)=\frac{1}{2}\mathbb E[||x-\tilde Z||^2],
\]</span> where <span class="math inline">\(\mathbb E(\cdot)\)</span> denotes the expectation with respect to the distribution of <span class="math inline">\(Z\)</span>, i.e., <span class="math display">\[
E[||x-\tilde Z||^2]=\int ||x-\tilde Z(w)||^2dP(w).
\]</span> If <span class="math inline">\(Z\)</span> is a discrete random variable, then <span class="math display">\[
E[||x-\tilde Z||^2]=\sum_{i=1}^Np_i||x-a_i||^2.
\]</span> In practice, to solve this problem, one may draw <span class="math inline">\(m\)</span> independent identically distributed samples of <span class="math inline">\(\tilde Z\)</span>, say <span class="math inline">\(S=\{z_1,\dots,z_m\}\)</span>, and then solve <span class="math display">\[
\min_{x\in\mathbb R^n}F_S(x)=\frac{1}{m}\sum_{i=1}^m \frac{1}{2}||x-z_i||^2.
\]</span> The corresponding optimality condition is <span class="math display">\[
0=\nabla F_S(x)=\frac{1}{m}\sum_{i=1}^m(x-z_i)\implies x=\frac{1}{m}\sum_{i=1}^mz_i.
\]</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
        <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block home">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/24/Nonlinear Optimization/3.9 Cones/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Orange+Dragon">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Orandragon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2019/08/24/Nonlinear Optimization/3.9 Cones/" class="post-title-link" itemprop="url">3. Basic Convex Analysis (2)</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-08-24 23:28:41" itemprop="dateCreated datePublished" datetime="2019-08-24T23:28:41+08:00">2019-08-24</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-09-25 17:03:04" itemprop="dateModified" datetime="2019-09-25T17:03:04+08:00">2019-09-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Nonlinear-Optimization/" itemprop="url" rel="index"><span itemprop="name">Nonlinear Optimization</span></a></span>

                
                
              
            </span>
          

          
            <span id="/2019/08/24/Nonlinear Optimization/3.9 Cones/" class="post-meta-item leancloud_visitors" data-flag-title="3. Basic Convex Analysis (2)" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2019/08/24/Nonlinear Optimization/3.9 Cones/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2019/08/24/Nonlinear Optimization/3.9 Cones/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="thanks-for-the-module-ma5268-nus-and-prof.-toh">Thanks for the module MA5268 NUS and Prof. Toh</h1>
<h2 id="cones">3.9 Cones</h2>
<p><strong>Definition</strong></p>
<p>Consider <span class="math inline">\(p:\mathcal E\rightarrow (-\infty,\infty]\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(p\)</span> is said to be a <strong>proper function</strong> if <span class="math inline">\(p(x)\)</span> is not identically equal to <span class="math inline">\(\infty\)</span>.</li>
<li><span class="math inline">\(p\)</span> is said to be <strong>closed</strong> if its epi-graph <span class="math inline">\(\text{epi}(p):=\{(\alpha,x)\in \mathbb R\times \mathcal E\;|\; p(x)\le\alpha\}\)</span> is a closed subset of <span class="math inline">\(\mathbb R\times \mathcal E\)</span>.</li>
<li>The <strong>domain</strong> of <span class="math inline">\(p\)</span> is defined to be the set</li>
</ol>
<p><span class="math display">\[
\text{dom} (p)=\{x\in \mathbb R^n\;|\;f(x)&lt;\infty\}.
\]</span></p>
<p>For a nonempty closed set <span class="math inline">\(C\subset \mathbb R^n\)</span>, <span class="math inline">\(\delta_C\)</span> is a proper closed function.</p>
<p><strong>Definition</strong></p>
<ol type="1">
<li><p><strong>Linearity space</strong></p>
<p>Let <span class="math inline">\(C\subset \mathbb R^n\)</span> be a closed convex cone. The linearity space of <span class="math inline">\(C\)</span> is the set <span class="math display">\[
\text{lin} (C)=C\cap(-C).
\]</span></p>
<p>It is the largest linear subspace of <span class="math inline">\(\mathcal E\)</span> contained in C.</p></li>
<li><p><strong>Dual and polar cone</strong></p>
<p>The dual of any cone <span class="math inline">\(S\)</span> (not necessarily convex) is defined by</p></li>
</ol>
<p><span class="math display">\[
   S^* =\{y\in\mathcal E\;|\;\langle x,y\rangle\ge 0\;\forall x\in S\}.
\]</span> The polar cone of <span class="math inline">\(S\)</span> is <span class="math inline">\(S^o=-S^*\)</span>.</p>
<ol start="3" type="1">
<li><p><strong>Self-dual</strong></p>
<p>Let C be a cone in <span class="math inline">\(\mathcal E\)</span>. If <span class="math inline">\(C^*=C\)</span>, then <span class="math inline">\(C\)</span> is said to be self-dual.</p></li>
</ol>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(C\)</span> be a cone in <span class="math inline">\(\mathcal E\)</span>.</p>
<ol type="1">
<li><span class="math inline">\(C^*\)</span> is a closed convex cone.</li>
<li>If <span class="math inline">\(C\)</span> is a <strong>nonempty closed convex</strong> cone, then <span class="math inline">\((C^*)^*=C\)</span>.</li>
</ol>
<p>Proof.</p>
<ol type="1">
<li>Suppose <span class="math inline">\(u,v \in C^*\)</span> and <span class="math inline">\(\alpha,\beta\in \mathbb R_+\)</span>. <span class="math display">\[
 \langle x,\alpha u+\beta v\rangle=\alpha \langle x,u\rangle+\beta \langle x,y\rangle\ge 0
 \]</span> Q.E.D.</li>
</ol>
<p><strong>Example</strong></p>
<p>If <span class="math inline">\(\mathcal E=\mathbb R^n\)</span>, and <span class="math inline">\(C=\mathbb R_+^n\)</span>. Then</p>
<ol type="1">
<li><span class="math inline">\(\text{aff} (C)=\mathbb R^n\)</span></li>
<li><span class="math inline">\(\text{lin} (C)=\{0\}\)</span></li>
<li><span class="math inline">\(C^*=\mathbb R_+^n\)</span>.</li>
</ol>
<p><strong>Examples</strong></p>
<ol type="1">
<li><p>Let <span class="math inline">\(\mathcal E=\mathbb S^n\)</span> be the space of <span class="math inline">\(n\times n\)</span> real symmetric matrices. Then the space <span class="math inline">\(\mathbb S_+^n\)</span> of <span class="math inline">\(n\times n\)</span> symmetric positive semidefinite matrices is a closed convex cone in <span class="math inline">\(\mathbb S^n\)</span>.</p>
<p>Proof of convexity.</p>
<p>For any <span class="math inline">\(X,Y\in \mathbb S_+^n\)</span> and <span class="math inline">\(\alpha,\beta\in \mathbb R_+\)</span>, we have <span class="math inline">\(\alpha X+\beta Y\in \mathbb S_+^n\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\mathcal E =\mathbb R^n\)</span>. The set <span class="math display">\[
 C=\{(x_1;\bar x)\in \mathbb R^n\;|\; ||\bar x||\le x_1\}
 \]</span> is a cone, which is known as a <strong>second-order cone</strong>.</p>
<p>Proof of convexity.</p>
<p>For any <span class="math inline">\((x_1;\bar x),(y_1;\bar y)\in C\)</span> and <span class="math inline">\(\alpha,\beta \in\mathbb R_+\)</span>, we have <span class="math display">\[
    \alpha (x_1;\bar x)+\beta (y_1;\bar y)=(\alpha x_1+\beta y_1,\alpha\bar x+\beta \bar y).
\]</span> Then we check <span class="math display">\[
    ||\alpha \bar x+\beta \bar y||\le \alpha||\bar x||+\beta ||\bar y||\le \alpha x_1+\beta y_1.
\]</span> Q.E.D.</p>
<p>Since <span class="math inline">\(f(x)=||x||\)</span> is convex, <span class="math inline">\(C\)</span> is its epigraph and therefore convex. Q.E.D.</p></li>
<li><p>A cone that is formed by the intersection of finitely many half space is known as a <strong>polyhedral cone</strong>, and it has the form: <span class="math inline">\(\{x\in\mathbb R^n\;|\;Ax\le 0\}\)</span>, where <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>.</p>
<p>Proof of convexity.</p>
<p>It is straightforward to show the convexity.</p></li>
<li><p><strong>Exponential cone</strong>. Consider the set <span class="math display">\[
    K=\{(x,y,z)\in\mathbb R^3\;|\;y\exp(x/y)\le z,y&gt;0\}.
 \]</span> The exponential cone is <span class="math inline">\(C=\text{cl }(K)\)</span>.</p>
<p>Proof convexity.</p>
<p>We define <span class="math inline">\(f(x,y)=y\exp(x/y)\)</span>, then we find the Hessian matrix is positive definite for all <span class="math inline">\(y&gt;0\)</span>.</p>
<p>Since <span class="math inline">\(y\exp(x/y)\le z\)</span> is the epigraph of the convex function, and closure of a convex set is convex. Then we finish the proof.</p></li>
<li><p><strong>Power cone</strong>. For <span class="math inline">\(\alpha =(\alpha_1;\dotsm,\alpha_n)&gt;0\)</span> and <span class="math inline">\(\sum_{i=1}^n\alpha_i=1\)</span>, <span class="math display">\[
 K_\alpha = \{(x;y)\in\mathbb R_+^n\times \mathbb R\;|\;|y|\le x_1^{\alpha_1}\dotsm x_n^{\alpha_n}\}.
 \]</span></p></li>
<li><p><strong>Spectral cone</strong> and <strong>nuclear-norm cone</strong>.</p>
<p>Suppose <span class="math inline">\(X\in \mathbb R^{m\times n}\)</span> with <span class="math inline">\(m\le n\)</span>. Let the singular value decomposition of <span class="math inline">\(X\)</span> be <span class="math display">\[
X=U[\Sigma, 0]V^T
\]</span> where <span class="math inline">\(U\in \mathbb R^{n\times n}\)</span> and <span class="math inline">\(V\in\mathbb R^{m\times m}\)</span> are unitary matrices, and <span class="math inline">\(\Sigma=\text{diag }(\sigma_1,\dots,\sigma_m)\)</span> with <span class="math inline">\(\sigma_1\ge \dotsm\ge \sigma_m\ge 0\)</span>. We can define the following cones: <span class="math display">\[
\mathcal K=\{(t,X)\in \mathbb R\times \mathbb R^{m\times n}\;|\;||X||_*\le t\}\\
   \mathcal C=\{(t,X)\in\mathbb R\times \mathbb R^{m\times n}\;|\; ||X||_2\le t\}
\]</span> where <span class="math inline">\(||X||_*=\sum_{i=1}^m\sigma _i(X)\)</span> and <span class="math inline">\(||X||_2=\sigma_1(X)\)</span>.</p>
<p>Proof of (<span class="math inline">\(\mathcal K^*=\mathcal C\)</span>).</p>
<p>Firstly, we will prove <span class="math inline">\(\mathcal C\subseteq \mathcal K^*\)</span>, where <span class="math display">\[
\begin{array}{rcl}
\mathcal K^*&amp;=&amp;\left\{(y,Y)\in\mathbb R\times\mathbb R^{m\times n}\;|\; \langle(x,X),(y,Y)\rangle \ge 0\; ,\forall (x,X)\in \mathcal K\right\}\\
&amp;=&amp;\left\{(y,Y)\in\mathbb R\times\mathbb R^{m\times n}\;|\; xy+\langle X,Y\rangle \ge 0\;, ||X||_*\le x\right\}.
\end{array}
\]</span> Assume <span class="math inline">\((y,Y)\in \mathcal C\)</span>, and then we have <span class="math display">\[
||Y||_2\le y\iff\sigma_1(Y)\le y.
\]</span> For any <span class="math inline">\((x,X)\in \mathcal K\)</span>, we have <span class="math display">\[
||X||_*\le x\iff \sigma_1(X)+\dotsm+\sigma_m(X)\le x.
\]</span> Since we have <span class="math display">\[
\langle -X,Y\rangle\le ||X||_*||Y||_2\le xy,
\]</span></p>
<p>then for any <span class="math inline">\((x,X)\in \mathcal K\)</span>, we have <span class="math display">\[
 \langle X,Y\rangle+xy\ge 0,
\]</span> which means <span class="math inline">\((y,Y)\in \mathcal K^*\)</span>.</p>
<p>Conversely, we will prove <span class="math inline">\(\mathcal K^*\subseteq \mathcal C\)</span>. We will prove this by contradiction. Assume <span class="math inline">\((y,Y)\in\mathcal K^*\)</span>, and define <span class="math inline">\(Y=U_y[\Sigma_y,0]V_y^T\)</span>, where <span class="math inline">\(U_y\)</span> and <span class="math inline">\(V_y\)</span> are unitary matrices and <span class="math inline">\(\Sigma_y=\text{diag}(\sigma_{y1},\dots,\sigma_{ym})\)</span> with <span class="math inline">\(\sigma_{y1}\ge \dotsm\ge \sigma_{ym}\ge 0\)</span>. Then for any <span class="math inline">\((x,X)\in \mathcal K\)</span>, i.e., <span class="math inline">\(||X||_*\le x\)</span>, we have <span class="math display">\[
 0\le xy+\langle X,Y\rangle.
\]</span></p>
<p>Assume there exists <span class="math inline">\((y,Y)\notin \mathcal C\)</span>, i.e., <span class="math inline">\(||Y||_2&gt;y\)</span>. We choose <span class="math inline">\(X^*=-[\text{diag}\{1,0,0\dotsm\},0]\)</span> and <span class="math inline">\(x^*=1\)</span>. It is straightforward to see that <span class="math display">\[
 ||X^*||_2=1=x^*,
\]</span> which means <span class="math inline">\((x^*,X^*)\in \mathcal K\)</span>. Then we have <span class="math display">\[
\begin{array}{rcl}
x^*y+\text{Tr}\left((X^*)^TY\right)&amp;=&amp;x^*y-\text{Tr}([\text{diag}\{1,0,0\dotsm\},0]^T U_y[\Sigma_y\;0]V_y^T)\\
 &amp;=&amp;x^*y-\text{Tr}([\text{diag}\{1,0,0\dotsm\},0]^T U_y[\Sigma_y\;0])\\
 &amp;=&amp;x^*y-\text{Tr}([\Sigma_y\;0][\text{diag}\{1,0,0\dotsm\},0]^T)\\
 &amp;=&amp;x^*y-\text{Tr}(\Sigma_y\text{diag}\{1,0,0\dotsm\})\\
 &amp;=&amp;y-||Y||_2\\
 &amp;&lt;&amp; 0,
 \end{array}
\]</span> which contradicts that <span class="math inline">\(0\le xy+\langle X,Y\rangle\)</span> for any <span class="math inline">\((x,X)\in \mathcal K\)</span>. Q.E.D.</p></li>
<li><p><strong>Copositive and completely positive cones</strong>.</p>
<ol type="1">
<li><p>Copositive cone: <span class="math inline">\(\text{COP}_n=\{X\in\mathbb S^n\;|\; v^TXv\ge 0\;\forall v\in \mathbb R_+^n\}\)</span></p></li>
<li><p>Completely positive cone:</p></li>
</ol>
<p><span class="math display">\[
\begin{array}{rcl}
CPP_n&amp;=&amp;\{X\in\mathbb S^n\;|\; X=VV^T\;\exists V\in\mathbb R_+^n\}\\
&amp;=&amp;\left\{\sum_{i=1}^r v_iv_i^T\;|\;v_i\in\mathbb R_+^n\;\forall i=1,\dotsm,r,\;r\in Z_+ \right\}
\end{array}
\]</span></p>
<p>Proof of (<span class="math inline">\(CPP_n^*=COP_n\)</span>)</p>
<p><span class="math display">\[
CPP_n^* =\{Y\in\mathbb S^n\;|\;\langle X,Y\rangle\ge 0\;\forall X\in CPP_n\}
\]</span> We first prove that <span class="math inline">\(COP_n\subseteq CPP_n^*\)</span>, <span class="math display">\[
    v^TYv\ge0\iff\langle Y,vv^T\rangle\ge 0\iff\langle Y,X\rangle \ge 0,\forall X\in CPP_n.
\]</span> That means <span class="math inline">\(Y\in CPP_n^*\)</span>. Since every step we have equal conditions, Q.E.D.</p></li>
<li><p>Doubly nonnegative cone.</p>
<p><span class="math inline">\(\mathbb D_n=\mathbb S_+^n\cap \mathcal N^n\)</span> where <span class="math inline">\(\mathcal N^n\)</span> is the set of symmetric <span class="math inline">\(n\times n\)</span> nonnegative matrices (a matrix in which all the elements are equal to or greater than zero). We can also define <span class="math inline">\(\mathbb D_n\)</span> as <span class="math display">\[
\mathbb D_n=\{X|X=X^T,\forall d\in \mathbb R^n,d^TXd\ge 0,X\ge 0\}.
\]</span> Proof. (<span class="math inline">\(\mathbb D_n^*=\mathbb S_+^n+\mathcal N^n\)</span>)</p>
<p>Firstly we prove that <span class="math inline">\(\mathbb S_+^n+\mathcal N^n\subseteq \mathbb D_n^*\)</span>.</p>
<p>If <span class="math inline">\(Y_1\in \mathbb S_+^n\)</span> and <span class="math inline">\(Y_2 \in \mathcal N^n\)</span>, then we have <span class="math inline">\(Y=Y_1+Y_2\in\mathbb S_+^n+\mathcal N^n\)</span>.</p>
<p>Then we have <span class="math display">\[
\langle X,Y\rangle=\langle X,Y_1+Y_2\rangle=\langle X,Y_1\rangle+\langle X,Y_2\rangle\ge 0.
\]</span> Therefore, <span class="math inline">\(\mathbb S_+^n+\mathcal N^n\subseteq \mathbb D_n^*\)</span>.</p>
<p>Then we prove $D_n^*S_+<sup>n+N</sup>n $.</p>
<p>If <span class="math inline">\(Y\in \mathbb D_n^*\)</span>, then we have <span class="math display">\[
\langle X,Y\rangle\ge 0,d^TXd\ge 0,X\ge0\\
\langle X,Y_1\rangle+ \langle X,Y_2\rangle\ge 0,d^TXd\ge 0,X\ge0.
\]</span> Since we have <span class="math inline">\(X\)</span> is positive definite, we can choose any <span class="math inline">\(Y_1\in \mathcal N^n\)</span> and any <span class="math inline">\(Y_2\in S_+^n\)</span>. Q.E.D.</p></li>
</ol>
<p><strong>Example</strong></p>
<p>Let <span class="math inline">\(C\)</span> be a closed convex cone in <span class="math inline">\(\mathcal E\)</span>. Show that for any <span class="math inline">\(z\in\mathcal E\)</span>, <span class="math display">\[
z-\Pi_C(z)=\Pi_{C^o}(z).
\]</span> Proof.</p>
<p>Let <span class="math inline">\(w=z-\Pi_C(z)\)</span>. Then we have <span class="math display">\[
\langle y-w,z-w\rangle\le 0\;\forall y\in{C^o},
\]</span> which also means <span class="math inline">\(w=\Pi_{C^o}(z)\)</span>.</p>
<h2 id="normal-cones">3.10 Normal cones</h2>
<p><strong>Definition</strong> (Normal cone)</p>
<p>Let <span class="math inline">\(C\subset \mathcal E\)</span> be a convex set and <span class="math inline">\(\bar x\in C\)</span>. The normal cone of <span class="math inline">\(C\)</span> at <span class="math inline">\(\bar x\in C\)</span> is defined by <span class="math display">\[
N_c(\bar x)=\{z\in \mathcal E\;|\;\langle z,x-\bar x\rangle\le 0\quad \forall x\in C\}
\]</span> By convention, we let <span class="math inline">\(N_C(\bar x)=\O\)</span> if <span class="math inline">\(\bar x\notin C\)</span>.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(C\subset \mathcal E\)</span> be a convex set and <span class="math inline">\(\bar x \in C\)</span>. Then</p>
<ol type="1">
<li><span class="math inline">\(N_C(\bar x)\)</span> is a closed convex cone.</li>
<li>If <span class="math inline">\(\bar x\in \text{int} (C)\)</span>, then <span class="math inline">\(N_C(\bar x)=\{0\}\)</span>.</li>
<li>If <span class="math inline">\(C\)</span> is also a cone, then <span class="math inline">\(N_C(\bar x)\subset C^o\)</span>.</li>
</ol>
<p>Proof.</p>
<ol type="1">
<li><p>If <span class="math inline">\(z_1,z_2\in N_c(\bar x)\)</span> and <span class="math inline">\(\alpha,\beta \in\mathbb R\)</span>, we have <span class="math display">\[
\langle \alpha z_1+\beta z_2,x-\bar x\rangle \le 0\;\forall x\in C.
\]</span> Q.E.D.</p></li>
<li><p>If <span class="math inline">\(\bar x\in \text{int} (C)\)</span>, then we have <span class="math inline">\(\varepsilon &gt;0\)</span>, such that the ball <span class="math inline">\(B_{\varepsilon}(\bar x)\subseteq C\)</span>. Then we can choose <span class="math inline">\(x=\bar x+\lambda z\in B_\varepsilon (\bar x)\)</span>, and we have <span class="math display">\[
\langle z,\lambda z\rangle \ge0.
\]</span> Therefore, <span class="math inline">\(z=0\)</span>.</p></li>
<li><p>If <span class="math inline">\(C\)</span> is a cone, then we have <span class="math inline">\(x+\bar x\in C\;\forall x\in C\)</span>. Thus for any <span class="math inline">\(z\in N_C(\bar x)\)</span>, we have <span class="math display">\[
\langle z,x\rangle = \langle z,(x+\bar x)-\bar x\rangle\le 0\;\forall x\in C,
\]</span> Hence <span class="math inline">\(z\in C^o\)</span>. Q.E.D.</p></li>
</ol>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(C\subset \mathbb R^n\)</span> be a closed convex set. Then for any <span class="math inline">\(u,y\in C\)</span>, <span class="math display">\[
u\in N_C(y)\iff y=\Pi_C(y+u)
\]</span> Proof.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>Assume <span class="math inline">\(u\in N_C(y)\)</span>, i.e. <span class="math inline">\(\langle u,z-y\rangle \le 0\;\forall z\in C\)</span>, we need to prove <span class="math display">\[
\langle y+u-y,z-y\rangle \le 0\;\forall z\in C\\
\iff\\
\langle u,z-y\rangle\le \;\forall z\in C
\]</span> <span class="math inline">\(\Longleftarrow\)</span></p>
<p>Same as the above. Q.E.D.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(C_1,C_2 \subset \mathbb R^n\)</span> be convex sets such that <span class="math inline">\(\bar x\in C_1\cap C_2\)</span>.</p>
<ol type="1">
<li>It holds that <span class="math inline">\(N_{C_1}(\bar x)+N_{C_2}(\bar x)\subset N_{C_1\cap C_2}(\bar x)\)</span></li>
<li>If the following relative interior condition holds</li>
</ol>
<p><span class="math display">\[
\text{ri}(C_1)\cap \text{ri}(C_2)\neq \O
\]</span></p>
<p> then we also have <span class="math display">\[
N_{C_1\cap C_2}(\bar x)\subset N_{c_1}(\bar x)+N_{c_2}(\bar x)
\]</span></p>
<p>Proof.</p>
<ol type="1">
<li><p>Assume <span class="math inline">\(x_1\in N_{C_1}(\bar x)\)</span> and <span class="math inline">\(x_2\in N_{C_2}(\bar x)\)</span>. Then we need to prove that <span class="math inline">\(x_1+x_2\in N_{C_1\cap C_2}(\bar x)\)</span>. <span class="math display">\[
x_1\in N_{C_1}(\bar x)\iff \langle x_1,x-\bar x\rangle \le 0\;\forall x\in C_1\\
x_2\in N_{C_2}(\bar x)\iff \langle x_2,x-\bar x\rangle \le 0\;\forall x\in C_2
\]</span> We need to prove <span class="math display">\[
(x_1+x_2) \in N_{C_1\cap C_2}(\bar x)\iff \langle x_1+x_2,x-\bar x\rangle \le 0\;\forall x\in C_1\cap C_2
\]</span> It is straightforward to prove that.</p></li>
<li><p>Not done.</p></li>
</ol>
<h2 id="subgradients">3.11 Subgradients</h2>
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> be a convex function. A vector <span class="math inline">\(v\)</span> is a subgradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\in\text{dom}(f)\)</span> if <span class="math display">\[
f(z)\ge f(x)+\langle v,z-x\rangle \;\forall z\in\mathcal E.
\]</span> The set of all subgradients at <span class="math inline">\(x\)</span> is called the subdifferential of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>, and denoted as <span class="math inline">\(\partial f(x)\)</span>. By convention, <span class="math inline">\(\partial f(x)=\O\)</span> for any <span class="math inline">\(x\notin\text{dom} (f)\)</span>.</p>
<p><strong>Proposition</strong></p>
<p><span class="math inline">\(\partial f:\mathcal E\rightarrow \mathcal E\)</span> is a monotone mapping. That is <span class="math display">\[
\langle v-u,y-x\rangle \ge 0\;\forall u\in \partial f(x),v\in \partial f(y).
\]</span> Proof.</p>
<p>By the definition, we have <span class="math display">\[
f(y)\ge f(x)+\langle u,y-x\rangle \\
f(x)\ge f(y)-\langle v,y-x\rangle.
\]</span> It is straightforward to get proved.</p>
<p><strong>Example</strong></p>
<ol type="1">
<li><p>If <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span>, then <span class="math inline">\(\partial f(x)=\{\nabla f(x)\}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(C\)</span> be a convex subset of <span class="math inline">\(\mathbb R^n\)</span>. Then <span class="math display">\[
\partial \delta_C(x)=
\begin{cases}\O&amp;\text{if }x\notin C\\ N_C(x)&amp;\text{if }x\in C \end{cases}
\]</span></p></li>
<li><p>Let <span class="math inline">\(f(x)=||x||_1\)</span> for <span class="math inline">\(x\in \mathbb R^n\)</span>. Show that <span class="math display">\[
\partial f(0)=\{y\in\mathbb R^n\;|\; ||y||_\infty\le 1\}
\]</span> Proof.</p>
<p><span class="math inline">\(\implies\)</span></p>
<p>Assume <span class="math inline">\(||y||_\infty \le 1\)</span>. Then we need to prove <span class="math display">\[
f(z)\ge f(0)+\langle y,z-0\rangle\;\forall z\in\mathbb R^n\\
\iff\\
||z||_1\ge \langle y,z\rangle\;\forall z\in\mathbb R^n.
\]</span> Then <span class="math display">\[
\langle y,z\rangle =y_1z_1+\dotsm+y_nz_n\le |y_1z_1|+\dotsm|y_nz_n|\\
\le |y_1||z_1|+\dotsm+|y_n||z_n|\le |z_1|+\dotsm+|z_n|=||z||_1.
\]</span> <span class="math inline">\(\Longleftarrow\)</span></p>
<p>Assume <span class="math inline">\(||z||_1\ge \langle y,z\rangle\;\forall z\in \mathbb R^n\)</span>. We need to prove <span class="math inline">\(||y||_\infty\le 1\)</span>. We will prove by contradiction. Assume <span class="math inline">\(||y||_\infty &gt;1\)</span>.</p>
<p>(draft) Let <span class="math inline">\(z\)</span> be a set, the <span class="math inline">\(i\)</span>th element be <span class="math inline">\(\max (y)\)</span> where <span class="math inline">\(i\)</span> is the index of the max element in <span class="math inline">\(y\)</span>. Then we have contradiction.</p>
<p>Q.E.D.</p></li>
</ol>
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> be an extended real-valued function. Define epigraph and effective domain as <span class="math display">\[
\text{epi}f=\{(x,\mu)\in\mathcal E\times \mathbb R\;|\;f(x)\le \mu\}\\
\text{dom} f=\{x\in\mathcal E\;|\;f(x)&lt;\infty\}.
\]</span></p>
<ol type="1">
<li><span class="math inline">\(f\)</span> is said to be <strong>convex (closed)</strong> if <span class="math inline">\(\text{epi }f\)</span> is convex (closed).</li>
<li><span class="math inline">\(f\)</span> is said to be <strong>proper</strong> if <span class="math inline">\(\text{dom}f\)</span> is nonempty.</li>
<li><span class="math inline">\(f\)</span> is said to be positively homogeneous if <span class="math inline">\(f(\lambda x)=\lambda f(x)\)</span>for all <span class="math inline">\(x\in\mathcal E\)</span> and <span class="math inline">\(\lambda &gt;0\)</span>. Any norm function on <span class="math inline">\(\mathcal E\)</span> is positively homogenous.</li>
</ol>
<p><strong>Definition</strong></p>
<p>A function <span class="math inline">\(F:\mathcal E_1\rightarrow \mathcal E_2\)</span> is said to be locally Lipschitz continuous if for any open set <span class="math inline">\(\mathcal O\subset \mathcal E_1\)</span>, there exists a constant <span class="math inline">\(L\)</span> (depending on <span class="math inline">\(\mathcal O\)</span>) such that <span class="math display">\[
||F(x)-f(y)||\le L||x-y||\;\forall x,y\in\mathcal O.
\]</span> If <span class="math inline">\(\mathcal O=\mathcal E_1\)</span>, then <span class="math inline">\(F\)</span> is said to be globally Lipschitz continuous.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(f\)</span> be an extended real-valued proper convex function on <span class="math inline">\(\mathcal E\)</span>. Then <span class="math inline">\(f\)</span> is locally Lipschitz continuous on <span class="math inline">\(\text{ri}(\text{dom}(f))\)</span>, the relative interior of <span class="math inline">\(\text{dom}(f)\)</span>. Moreover, <span class="math inline">\(f\)</span> is Lipschitz continuous on any closed bounded subset of <span class="math inline">\(\text{ri}(\text{dom}(f))\)</span>.</p>
<p><strong>Theorem</strong></p>
<p>Let <span class="math inline">\(\mathcal O\)</span> be a open subset of <span class="math inline">\(\mathcal E_1\)</span> and <span class="math inline">\(F:\mathcal O\rightarrow \mathcal E_2\)</span> is locally Lipschitz on <span class="math inline">\(\mathcal O\)</span>, then <span class="math inline">\(F\)</span> is almost everywhere Frchet differentiable on <span class="math inline">\(\mathcal O\)</span>.</p>
<p><strong>Definition</strong></p>
<p>A function <span class="math inline">\(F:\mathcal E_1\rightarrow \mathcal E_2\)</span> is locally Lipschitz continuous. Let <span class="math display">\[
\mathcal D_F=\{x\in\mathcal E_1\;|\;F&#39;(x)\;\text{exists}\}
\]</span> The B-subdifferential of <span class="math inline">\(F\)</span> at <span class="math inline">\(x\)</span> is defined by <span class="math display">\[
\partial _BF(x)=\left\{\lim_{x^k(\in \mathcal D_F)\rightarrow x}F&#39;(x^k)\right\}.
\]</span> The Clarke generalized Jacobian is defined by <span class="math display">\[
\partial F(x)=\text{conv} (\partial _BF(x)).
\]</span> In mathematics, <strong>Clarke's generalized Jacobian</strong> is a generalization of the Jacobian matrix of a smooth function to non-smooth functions.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(f\)</span> be an extended real-valued convex function on <span class="math inline">\(\mathcal E\)</span>. Then</p>
<ol type="1">
<li><p>If <span class="math inline">\(f\)</span> is proper, then <span class="math inline">\(\text{ri}(\text{dom}(f))\neq \O\)</span> and <span class="math inline">\(\partial f(x)\neq \O\)</span> for any <span class="math inline">\(x\in \text{ri}(\text{dom}(f))\)</span>. Moreover, <span class="math inline">\(\partial f(x)\)</span> is bounded if and only if <span class="math inline">\(x\in\text{int}(\text{dom}(f))\)</span>.</p></li>
<li><p><span class="math inline">\(\partial f\)</span> is monotone, i.e., for any <span class="math inline">\(x,y\in\mathcal E\)</span>, <span class="math display">\[
\langle u-v,x-y\rangle \ge 0\;\forall u\in \partial f(x),v\in\partial f(y).
\]</span></p></li>
<li><p>Let <span class="math inline">\(f,g\)</span> be proper convex functions on <span class="math inline">\(\mathcal E\)</span>. If <span class="math inline">\(\text{ri}(\text{dom}(f))\cap\text{ri}(\text{dom}(g))\neq \O\)</span>, then <span class="math display">\[
\partial(f+g)(x)=\partial f(x)+\partial g(x)\;\forall x\in\mathcal E.
\]</span></p></li>
<li><p>If <span class="math inline">\(f\)</span> is closed and proper, then <span class="math inline">\(\min_{z\in\mathcal E}f(z)\)</span> is attained at <span class="math inline">\(x\)</span> if and only if <span class="math inline">\(0\in \partial f(x)\)</span>.</p></li>
<li><p>If <span class="math inline">\(f\)</span> is closed and proper, then <span class="math inline">\(\partial f\)</span> is upper semicontinuous, i.e. for any <span class="math inline">\(v_k\in\partial f(x_k)\)</span> s.t. <span class="math inline">\(v_k\rightarrow v\)</span> and <span class="math inline">\(x_k\rightarrow x\)</span>, we have <span class="math inline">\(v\in \partial f(x)\)</span>.</p></li>
<li><p>If <span class="math inline">\(f(x)\)</span> is finite, then <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(x\)</span> if and only if <span class="math inline">\(\partial f(x)\)</span> is a singleton.</p></li>
<li><p>If <span class="math inline">\(f\)</span> is convex and <span class="math inline">\(f(x)\)</span> is finite, then the directional derivative <span class="math inline">\(f&#39;(x;h)\)</span> exists for any <span class="math inline">\(h\in\mathcal E\)</span>.</p></li>
<li><p>If <span class="math inline">\(f\)</span> is proper, <span class="math inline">\(f&#39;(x;h)\)</span> is an upper semicontinuous function of <span class="math inline">\((x,h)\in \text{int}(\text{dom}(f))\times \mathcal E\)</span>. Moreover, given any <span class="math inline">\(x\in\text{int}(\text{dom}(f))\)</span> and any <span class="math inline">\(\varepsilon &gt;0\)</span>, there exists a <span class="math inline">\(\delta&gt;0\)</span> such that <span class="math display">\[
\partial f(z)\subset \partial f(x)+\varepsilon B\;,\forall z\in(x+\delta B).
\]</span></p></li>
</ol>
<h2 id="fenchel-conjugate">3.12 Fenchel conjugate</h2>
<p>Let <span class="math inline">\(f\)</span> be an extended real-valued function on <span class="math inline">\(\mathcal E\)</span>. Then we define Fenchel conjugate <span class="math display">\[
f^*(y)=\sup\{\langle y,x \rangle-f(x)\;|\;x\in \mathcal E\},\;y\in\mathcal E.
\]</span> <span class="math inline">\(f^*\)</span> is always closed and convex, even if <span class="math inline">\(f\)</span> is non-convex or not closed.</p>
<p><strong>Proposition</strong></p>
<p>Let <span class="math inline">\(f\)</span> be a closed proper convex function on <span class="math inline">\(\mathcal E\)</span>. For any <span class="math inline">\(x\in\mathcal E\)</span>, we have the following equivalent conditions for a vector <span class="math inline">\(x^* \in\mathcal E\)</span>:</p>
<ol type="1">
<li><span class="math inline">\(f(x)+f^*(x^*)=\langle x,x^*\rangle\)</span></li>
<li><span class="math inline">\(x^*\in \partial f(x)\)</span></li>
<li><span class="math inline">\(x\in \partial f^*(x^*)\)</span></li>
<li><span class="math inline">\(\langle x,x^*\rangle -f(x)=\max_{z\in \mathcal E}\{\langle z,x^*\rangle -f(z)\}\)</span></li>
<li><span class="math inline">\(\langle x,x^*\rangle -f^*(x^*)=\max_{z^*\in \mathcal E}\{\langle x,z^*\rangle -f^*(z^*)\}\)</span></li>
</ol>
<p><strong>Remark</strong></p>
<ol type="1">
<li><p>For a convex set <span class="math inline">\(C\)</span>, we have <span class="math display">\[
\delta_C^*(x)=\sup\{\langle x,y\rangle-\delta_C(y)\}=\sup\{\langle x,y\rangle\;|\;y\in C\}.
\]</span></p></li>
<li><p>If <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> is a closed proper convex function, then <span class="math inline">\((f^*)^*=f\)</span>.</p></li>
<li><p>If <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> is a closed proper convex function, positively homogeneous and <span class="math inline">\(f(0)=0\)</span>, then <span class="math display">\[
f^*=\delta_C,\;where\; C=\partial f(0).
\]</span></p></li>
</ol>
<p><strong>Example</strong></p>
<p>Let <span class="math inline">\(f(x)=||x||_\#\)</span> be any norm function defined on <span class="math inline">\(\mathcal E\)</span> and <span class="math inline">\(||\cdot||_*\)</span> be the dual norm of <span class="math inline">\(||\cdot||_\#\)</span>, i.e., for any <span class="math inline">\(x\in \mathcal E\)</span>, <span class="math inline">\(||x||_*=\sup_{y\in\mathcal E}\{\langle x,y \rangle\;|\;||y||_\#\le 1\}\)</span>. Since <span class="math inline">\(f\)</span> is a positively homogeneous closed convex function, <span class="math inline">\(f^*=\delta_C\)</span> where <span class="math display">\[
C=\partial f(0)=B_*=\{x\in\mathcal E\;|\;||x||_*\le 1\}.
\]</span> <strong>Example</strong></p>
<p>Let <span class="math inline">\(f:\mathbb R^n\rightarrow \mathbb R\)</span> be defined by <span class="math inline">\(f(x)=\max_{i=1,\dotsm,n}x_i\)</span>. Then <span class="math display">\[
S=\partial f(0)=\{x\in\mathbb R^n\;|\; \sum_{i=1}^nx_i=1,x\ge 0\}.
\]</span> Since <span class="math inline">\(f\)</span> is proper closed convex function, positively homogeneous and <span class="math inline">\(f(0)=0\)</span>, <span class="math inline">\(f^*=\delta_S\)</span>.</p>
<p><strong>Example</strong></p>
<p>Let <span class="math inline">\(g(x)=\max_{i=1,\dotsm,n}x_i\)</span> and <span class="math inline">\(f(x)=g(x)+\delta_{\mathbb R^n_+}(x)\)</span>. Clearly, <span class="math inline">\(f(\cdot)\)</span> is a positively homogeneous convex function. Then <span class="math display">\[
\partial f(0)=\partial g(0)+\partial \delta_{\mathbb R^n_+}(0)=S-\mathbb R_+^n=\{x\in\mathbb R^n\;|\;e^Tx^+\le 1\},
\]</span> where <span class="math inline">\(S\)</span> is defined in the above example and <span class="math inline">\(x_i^+=\max(x_i,0)\)</span>. Let <span class="math inline">\(x\in \mathbb R^{n}\)</span> be given. Define <span class="math inline">\(I_+=\{i\;|\;x_i\ge 0\}\)</span>, <span class="math inline">\(I_-=\{i\;|\;x_i&lt; 0\}\)</span>, and <span class="math inline">\(C=\{\xi \in R^{|I_+|}\;|\; e^T\xi\le 1,\xi \ge 0\}\)</span>. The projection <span class="math inline">\(\bar x=\Pi_{\partial f(0)}(x)\)</span> is given by <span class="math display">\[
\begin{array}{rCl}
\bar x_{I_+}&amp;=&amp;\Pi_C(x_{I_+})\\
\bar x_{I_-}&amp;=&amp;x_{I_-}
\end{array}
\]</span></p>
<h2 id="semismoothness-of-convex-functions">3.13 Semismoothness of convex functions</h2>
<p><strong>Definition 3.14</strong> (Semismoothness)</p>
<p>Let <span class="math inline">\(\Omega\)</span> be an open subset of <span class="math inline">\(\mathcal E\)</span>. Let <span class="math inline">\(f:\Omega\rightarrow \mathbb R\)</span> be a locally Lipschitz continuous function (not necessarily convex).</p>
<ol type="1">
<li><p><span class="math inline">\(f\)</span> is said to be semismooth at <span class="math inline">\(x\)</span> if it is directionally differentiable at <span class="math inline">\(x\)</span> and <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle = O(||h||)\;\forall h\rightarrow 0,x+h\in \mathcal D_f
\]</span> where <span class="math inline">\(\mathcal D_f = \{y\in\Omega\;|\; f \text{ is differentiable at }y\}\)</span>.</p></li>
<li><p>Moreover, <span class="math inline">\(f\)</span> is said to be strongly semismooth at <span class="math inline">\(x\)</span> if <span class="math display">\[
f(x+h)-f(x)-\langle \nabla f(x+h),h\rangle = O(||h||^2)\;\forall h\rightarrow 0,x+h\in \mathcal D_f
\]</span></p></li>
</ol>
<p><strong>Theorem</strong></p>
<p>Any convex function <span class="math inline">\(f:\mathcal E\rightarrow \mathbb R\)</span> is semismooth.</p>
<p><strong>Remark</strong> Piecewise smooth functions are semismooth functions.</p>
<h2 id="moreau-yosida-my-regularization-and-proximal-mapping">3.14 Moreau-Yosida (MY) regularization and proximal mapping</h2>
<p>Let <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> be a closed proper convex function function.</p>
<p><strong>MY regularization</strong> of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>, <span class="math display">\[
M_f(x)=\min_{y\in \mathcal E}\left\{f(y)+\frac{1}{2}||y-x||^2\right\}.
\]</span> <strong>Proximal mapping</strong> of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>, <span class="math display">\[
P_f(x)=\arg\min_{y\in\mathcal E} \left\{f(y)+\frac{1}{2}||y-x||^2\right\}
\]</span> We also have <span class="math display">\[
P_{\lambda f}(x)=\arg\min_{y\in\mathcal E}\left\{f(y)+\frac{1}{2\lambda}||y-x||^2\right\}.
\]</span> And <span class="math display">\[
M_{\lambda f}(x)=\min_{y\in \mathcal E}\left\{f(y)+\frac{1}{2\lambda}||y-x||^2\right\}.
\]</span> <strong>MY regularization</strong> is also referred to as the <strong>Moreau envelope</strong> of <span class="math inline">\(f\)</span> with parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>The Moreau envelope <span class="math inline">\(M_f\)</span> is essentially a <strong>smoothed</strong> or <strong>regularized</strong> form of <span class="math inline">\(f\)</span>, it has <strong>domain</strong> <span class="math inline">\(\mathbb R^n\)</span> even when <span class="math inline">\(f\)</span> does not, and it is <strong>continuously differentiable</strong>, even when <span class="math inline">\(f\)</span> is not. In addition, the sets of minimizer of <span class="math inline">\(f\)</span> and <span class="math inline">\(M_f\)</span> are the same. The problems of minimizing <span class="math inline">\(f\)</span> and <span class="math inline">\(M_f\)</span> are thus equivalent, and the latter is always a smooth optimization problem (<span class="math inline">\(M_f\)</span> may be difficult to evaluate). To see why <span class="math inline">\(M_f\)</span> is a smoothed form of <span class="math inline">\(f\)</span>, we have <span class="math display">\[
M_f=(f^*+(1/2)||\cdot||^2_2)^*.
\]</span> Because we have <span class="math inline">\(M_f^{**}=M_f\)</span>, it can be interpreted as obtaining a smooth approximation to a function by taking its conjugate, adding regularization, and then taking the conjugate again. With no regularization, it would simply give the original function; with the quadratic regularization, it gives a smooth approximation.</p>
<p>The <strong>proximal operator</strong> and <strong>Moreau envelope</strong> of <span class="math inline">\(f\)</span> share many relationships. For example, <span class="math inline">\(P_f\)</span> returns the (unique) point that actually achieves the infimum that defines <span class="math inline">\(M_f\)</span>, i.e., <span class="math display">\[
M_f(x)=f(P_f(x))+(1/2)||x-P_f(x)||_2^2.
\]</span> In addition, the gradient of Moreau envelope is given by <span class="math display">\[
\nabla M_{\lambda f}(x)=(1/\lambda)(x-P_{\lambda f}(x)).
\]</span> This can be rewritten as <span class="math display">\[
P_{\lambda f}(x)=x-\lambda \nabla M_{\lambda f}(x),
\]</span> which shows that <span class="math inline">\(P_{\lambda f}\)</span> can be viewed as a gradient step, with step size <span class="math inline">\(\lambda\)</span>, for minimizing <span class="math inline">\(M_{\lambda f}\)</span> (which has the same minimizers as <span class="math inline">\(f\)</span>).</p>
<p>The definition indicates that <span class="math inline">\(P_f(x)\)</span> is a point that compromises between minimizing <span class="math inline">\(f\)</span> and being near to <span class="math inline">\(x\)</span>. For this reason, <span class="math inline">\(P_f(x)\)</span> is sometimes called a <strong>proximal point</strong> of <span class="math inline">\(x\)</span> with respect to <span class="math inline">\(f\)</span>. In <span class="math inline">\(P_{\lambda f}\)</span>, the parameter <span class="math inline">\(\lambda\)</span> can be interpreted as a relative weight or trade-off parameter between these terms.</p>
<p>There are many useful properties for the proximal operator.</p>
<p><strong>Separable sum</strong></p>
<p>If <span class="math inline">\(f\)</span> is separable across two variables, so <span class="math inline">\(f(x,y)=\varphi(x)+\psi(y)\)</span>, then <span class="math display">\[
P_f(v,w)=(P_\varphi(v),P_\psi(w)).
\]</span> <strong>Basic operations</strong></p>
<p>Postcomposition</p>
<p>If <span class="math inline">\(f(x)=\alpha \varphi(x)+b\)</span>, with <span class="math inline">\(\alpha &gt;0\)</span>, then <span class="math display">\[
P_{\lambda f}(v)=P_{\alpha \lambda \varphi}(v).
\]</span> Precomposition</p>
<p>If <span class="math inline">\(f(x)=\varphi (\alpha x+b)\)</span>, with <span class="math inline">\(\alpha\neq 0\)</span>, then <span class="math display">\[
P_{\lambda f}(v)=\frac{1}{\alpha}\left(P_{\alpha^2 \lambda \varphi}(\alpha v+b)-b\right).
\]</span> If <span class="math inline">\(f(x)=\varphi(Qx)\)</span>, where <span class="math inline">\(Q\)</span> is orthogonal <span class="math inline">\(QQ^T=Q^TQ=I\)</span>, then <span class="math display">\[
P_{\lambda f}(v)=Q^TP_{\lambda\varphi}(Qv).
\]</span> Affine addition</p>
<p>If <span class="math inline">\(f(x)=\varphi(x)+a^Tx+b\)</span>, then <span class="math display">\[
P_{\lambda f}(v)=P_{\lambda \varphi}(v-\lambda a).
\]</span> Regularization</p>
<p>If <span class="math inline">\(f(x)=\varphi(x)+(\rho/2)||x-a||_2^2\)</span>, then <span class="math display">\[
P_{\lambda f}(v)=P_{\tilde \lambda \varphi}\left(\left(\tilde \lambda/\lambda\right)v+(\rho\tilde \lambda)a\right)
\]</span> where <span class="math inline">\(\tilde \lambda=\lambda/(1+\lambda \rho)\)</span>.</p>
<p><strong>Fixed point algorithms</strong></p>
<p>The point <span class="math inline">\(x^*\)</span> minimizes <span class="math inline">\(f\)</span> if and only if <span class="math display">\[
x^*=P_f(x^*),
\]</span> i.e., if <span class="math inline">\(x^*\)</span> is a fixed point of <span class="math inline">\(P_f\)</span>. This fundamental property gives a link between proximal operator and fixed point theory. Many proximal algorithms for optimization can be interpreted as methods for finding fixed points of appropriate operators.</p>
<p>Since minimizers of <span class="math inline">\(f\)</span> are fixed points of <span class="math inline">\(P_f\)</span>, we can minimize <span class="math inline">\(f\)</span> by finding a fixed point of its proximal operator, we can minimize <span class="math inline">\(f\)</span> by finding a fixe point of its proximal operator. If <span class="math inline">\(P_f\)</span> were a <strong>contraction</strong>, i.e. it is Lipschitz continuous with constant less than 1, repeatedly applying <span class="math inline">\(P_f\)</span> would find a fixed point. It turns out that while <span class="math inline">\(P_f\)</span> need not be a contraction (unless <span class="math inline">\(f\)</span> is strongly convex), it does have a different property, <strong>firm nonexpansiveness</strong>, sufficient for fixed point iteration, <span class="math display">\[
||P_f(x)-P_y(y)||^2_2\le (x-y)^T(P_f(x)-P_f(y))
\]</span> for all <span class="math inline">\(x,y\in\mathbb R^n\)</span>.</p>
<p>Firmly nonexpansive operator are special case if <strong>nonexpansive operators</strong> (those that are Lipschitz continuous with constant 1). Iteration of a general nonexpansiveness operator need not converge to a fixed point (consider operators like <span class="math inline">\(-I\)</span>). However, it turns out that if <span class="math inline">\(N\)</span> is nonexpansive, then the operator <span class="math inline">\(T=(1-\alpha)I+\alpha N\)</span>, where <span class="math inline">\(\alpha \in(0,1)\)</span>, has the same fixed point as <span class="math inline">\(N\)</span> and simple iteration of <span class="math inline">\(T\)</span> will converge to a fixed point of <span class="math inline">\(T\)</span> (and thus <span class="math inline">\(N\)</span>), i.e., the sequence <span class="math display">\[
x^{k+1}=(1-\alpha) x^k+\alpha N(x^k)
\]</span> will converge to a fixed point of <span class="math inline">\(N\)</span>, i.e., <strong>Damped iteration</strong> of a nonexpansive operator will converge to one of its fixed points.</p>
<p>Operators that can denoted in the form <span class="math inline">\((1-\alpha)I+\alpha N\)</span>, where <span class="math inline">\(N\)</span> is nonexpansive and <span class="math inline">\(\alpha \in (0,1)\)</span>, are called <span class="math inline">\(\alpha\)</span><strong>-averaged operators</strong>. Firmly nonexpansive operators are averaged, indeed, the <span class="math inline">\((1/2)\)</span>-averaged operators. In summary, both <strong>contractions and firm nonexpansions</strong> are subsets of the class of averaged operators, which in turn are a subset of all nonexpansive operators.</p>
<p>Averaged operators are useful because they satisfy some properties that are desirable in devising fixed point methods, and because they are a common parent of contractions and firm nonexpansions. The class of averaged operators is closed under composition, unlike the firm nonexpansions, i.e., the composition of firmly nonexpansive operators need not be firmly nonexpansive but is always averaged. In addition, simple iteration of an averaged operator will converge to a fixed point if one exists. <span class="math display">\[
x^{k+1}=T(x^k)
\]</span> with arbitrary <span class="math inline">\(x^0\)</span>. Then <span class="math inline">\(||T(x^k)-x^k||\rightarrow 0\)</span> as <span class="math inline">\(k\rightarrow 0\)</span> and <span class="math inline">\(x^k\)</span> converges to a fixed point of <span class="math inline">\(T\)</span>. This immediately suggests the simplest proximal method, <span class="math display">\[
x^{k+1}=P_{\lambda f} (x^k),
\]</span> which is called <strong>proximal minimization</strong> or the <strong>proximal point algorithm</strong>.</p>
<p><strong>Proximal average</strong></p>
<p>Let <span class="math inline">\(f_1,f_2,\dotsm,f_m\)</span> be closed proper convex functions. Then we have that <span class="math display">\[
\frac{1}{m}\sum_{i=1}^m P_{f_i}=P_g,
\]</span> where <span class="math inline">\(g\)</span> is a function called the <strong>proximal average</strong> of <span class="math inline">\(f_1,f_2,\dotsm,f_m\)</span>. In the other words, the average of the proximal operators of a set of functions is itself the proximal operator of some function, and this function is called the <strong>proximal average</strong>.</p>
<p><strong>Resolvent of subdifferential operator</strong></p>
<p>We can view the subdifferential operator <span class="math inline">\(\partial f\)</span> of a closed proper convex function <span class="math inline">\(f\)</span> as a <strong>point-to-set mapping</strong> or a <strong>relation</strong> on <span class="math inline">\(\mathbb R^n\)</span>, i.e., <span class="math inline">\(\partial f\)</span> takes each point <span class="math inline">\(x\in\text{dom}f\)</span> to the set <span class="math inline">\(\partial f(x)\)</span>. Any point <span class="math inline">\(y\in\partial f(x)\)</span> is called a subgradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span>. We refer to the (point-to-point) mapping <span class="math inline">\(\nabla f\)</span> from <span class="math inline">\(x\in \text{dom} f\)</span> as the <strong>gradient mapping</strong>.</p>
<p><strong>Proposition</strong></p>
<p>The proximal operator <span class="math inline">\(P_{\lambda f}\)</span> and the differential operator <span class="math inline">\(\partial f\)</span> are related as follows, <span class="math display">\[
P_{\lambda f}=(I+\lambda \partial f)^{-1}.
\]</span> The (point-to-point) mapping <span class="math inline">\((I+\lambda \partial f)^{-1}\)</span> is called the resolvent of the operator <span class="math inline">\(\partial f\)</span> with parameter <span class="math inline">\(\lambda &gt;0\)</span>, so the proximal operator is the resolvent of the subdifferential operator. This relation has domain <span class="math inline">\(\mathbb R^n\)</span>, is single-valued, and so is a <strong>function</strong>, even though <span class="math inline">\(\partial f\)</span> is not.</p>
<p>Proof.</p>
<p>We assume that <span class="math inline">\(f\)</span> is subdifferentiable on its domain. By definition if <span class="math inline">\(z\in(I+\lambda \partial f)^{-1}(x)\)</span>, then <span class="math display">\[
x\in(I+\lambda \partial f)(z)=z+\lambda \partial f(z).
\]</span> This can be expressed as <span class="math display">\[
0\in \partial f(z)+(1/\lambda )(z-x),
\]</span> which can be rewritten as <span class="math display">\[
0\in\partial_z\left(f(z)+(1/2\lambda)||z-x||_2^2\right).
\]</span> This is the necessary and sufficient condition for <span class="math inline">\(z\)</span> to minimize the strongly convex function, which is <span class="math display">\[
z=\arg\min_u\left(f(u)+(1/2\lambda)||u-x||_2^2\right).
\]</span> Therefore, we have <span class="math inline">\(z=P_{\lambda f}(x)\)</span> and, in particular, that <span class="math inline">\((I+\lambda \partial f)^{-1}\)</span> is single-valued.</p>
<p><strong>Modified gradient step</strong></p>
<p>We have already seen that <span class="math display">\[
P_{\lambda f}(x)=x-\lambda \nabla M_{\lambda f}(x),
\]</span> i.e., <span class="math inline">\(P_{\lambda f}\)</span> is a gradient step for minimizing the Moreau envelope of <span class="math inline">\(f\)</span> with step size <span class="math inline">\(\lambda\)</span>.</p>
<p>If <span class="math inline">\(f\)</span> is twice differentiable at <span class="math inline">\(x\)</span>, with <span class="math inline">\(\nabla f(x)\succ0\)</span>, then as <span class="math inline">\(\lambda \rightarrow 0\)</span>, we have <span class="math display">\[
P_{\lambda f}(x)=(I+\lambda \nabla f)^{-1}(x)=x-\lambda \nabla f(x)+o(\lambda).
\]</span> In other words, for small <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(P_{\lambda f}(x)\)</span> converges to a gradient step in <span class="math inline">\(f\)</span> with step length <span class="math inline">\(\lambda\)</span>.</p>
<p><strong>Trust region problem</strong></p>
<p>Solution of the trust region problem can be transferred to a solution of the proximal problem. (See &quot;Proximal Algorithm&quot; - Boyd).</p>
<p><strong>Proposition</strong></p>
<ol type="1">
<li><span class="math inline">\(P_f(x)\)</span> exists and is unique.</li>
<li><span class="math inline">\(M_f(x)\le f(x)\)</span> for all <span class="math inline">\(x\in\mathcal E\)</span>.</li>
</ol>
<p><strong>Proposition</strong> (IMPORTANT)</p>
<p>Let <span class="math inline">\(f:\mathcal E\rightarrow (-\infty,\infty]\)</span> be a closed proper convex function. Then <span class="math display">\[
\arg\min\{f(x)\;|\; x\in\mathcal E\}=\arg\min\{M_f(x)\;|\; x\in\mathcal E\}.
\]</span> Proof.</p>
<p>Suppose <span class="math inline">\(x^*\)</span> is a minimizer of <span class="math inline">\(f\)</span>. Then <span class="math inline">\(f(x^*)\le f(x)\)</span> for all <span class="math inline">\(x\in\mathcal E\)</span>. Thus for any <span class="math inline">\(x\mathcal \in E\)</span>, <span class="math display">\[
M_f(x^*)=\min_{y\in \mathcal E}\{f(y)+\frac{1}{2}||y-x^*||^2\}\\
\le f(x^*)\;\;(y=x^*)\\
\le f(y)+\frac{1}{2}||y-x||^2\;\forall y\in\mathcal E 
\]</span> That means <span class="math inline">\(M_f(x^*)\le \min_{y\in\mathcal E }\{f(y)+\frac{1}{2}||y-x||^2\}=M_f(x)\)</span>.</p>
<p>Conversely, suppose <span class="math inline">\(x^*\)</span> is a minimizer of <span class="math inline">\(M_f\)</span>. Then <span class="math display">\[
f(P_f(x^*))+\frac{1}{2}||P_f(x^*)-x^*||^2=M_f(x^*)\le M_f(x)\le f(x).
\]</span> Let <span class="math inline">\(x=P_f(x^*)\)</span>, then we have <span class="math inline">\(P_f(x^*)=x^*\)</span>. Q.E.D.</p>
<p><strong>Theorem</strong> (Moreau decomposition)</p>
<p>Let <span class="math inline">\(f\)</span> be closed proper convex and <span class="math inline">\(f^*\)</span> be its conjugate. Then <span class="math display">\[
x=P_f(x)+P_{f^*} (x)\quad \forall x\in\mathcal E\\
\frac{1}{2}||x\|^2=M_f(x)+M_{f^*} (x).
\]</span> For any <span class="math inline">\(t&gt;0\)</span>, <span class="math display">\[
x=P_{tf}(x)+tP_{f^*/t}(x/t)\quad \forall x\in\mathcal E\\
\frac{1}{2}||x||^2=M_{tf}(x)+t^2M_{f^*/t}(x/t).
\]</span> <strong>Example</strong></p>
<p>Let <span class="math inline">\(f(x)=\lambda ||x||_1\)</span> for <span class="math inline">\(x\in\mathbb R^n\)</span>. Then <span class="math inline">\(f^*(z)=\delta_C(z)\)</span> where <span class="math inline">\(C=\partial f(0)=\{z\in\mathbb R^n\;|\;||z||_\infty\le \lambda\}\)</span>.</p>
<p>Proof.</p>
<p><span class="math inline">\(f(x)\)</span> is a closed proper convex function, positively homogeneous and <span class="math inline">\(f(0)=0\)</span>, then <span class="math display">\[
f^*=\delta_C,\;where\; C=\partial f(0).
\]</span> For any <span class="math inline">\(v\in\partial f(0)\)</span>, we have <span class="math display">\[
f(z)\ge f(0)+\langle v,z-0\rangle \;\forall z\\
=\langle v,z\rangle \;\forall z\\
\iff\\
\lambda ||z||_1\ge \langle v,z\rangle\;\forall z\\
\iff\\
||v||_\infty \le \lambda.
\]</span> <strong>Example</strong></p>
<p>Let <span class="math inline">\(C\subset \mathcal E\)</span> be closed and convex. For <span class="math inline">\(f=\delta _C\)</span>, <span class="math display">\[
P_{\delta _C}(x)=\arg\min_{y\in\mathcal E}\{\delta_C(y)+\frac{1}{2}||y-x||\}=\arg\min_{y\in C}\frac{1}{2}||y-x||^2=\Pi_C(x).
\]</span> Suppose <span class="math inline">\(C=\mathbb S_+^n\)</span>, the cone of <span class="math inline">\(n\times n\)</span> symmetric PSD matrices. Then <span class="math display">\[
\Pi_C(x)=Q\text{diag} (d_+)Q^T. (PSD)
\]</span> <strong>Theorem</strong></p>
<ol type="1">
<li><p><span class="math inline">\(P_f\)</span> and <span class="math inline">\(Q_f:I-P_f\)</span> are firmly nonexpansive, i.e., <span class="math display">\[
||P_f(x)-P_f(y)||^2\le \langle P_f(x)-P_f(y),x-y\rangle \quad \forall x,y\in\mathcal E\\
||Q_f(x)-Q_f(y)||^2\le \langle Q_f(x)-Q_f(y),x-y\rangle \quad \forall x,y\in\mathcal E.
\]</span> Hence <span class="math inline">\(||P_f(x)-P_f(y)||\le ||x-y||\)</span>. That is, <span class="math inline">\(P_f\)</span> is Lipschitz continuous with modules 1. The same statement also holds for <span class="math inline">\(Q_f\)</span>.</p></li>
<li><span class="math inline">\(M_f\)</span> is continuously differentiable and <span class="math inline">\(\nabla M_f(x)=x-P_f(x)\;\forall x\in\mathcal E\)</span>.</li>
<li><p><span class="math inline">\(\nabla M_{tf}(x)=tP_{f^*/t}(x/t)\)</span> and <span class="math inline">\(\nabla M_{f^*/t}(x)=t^{-1}P_{tf}(tx)\)</span>.</p></li>
</ol>
<p><strong>Proposition</strong> Let <span class="math inline">\(f\)</span> be a closed proper convex function on <span class="math inline">\(\mathcal E\)</span>. For any <span class="math inline">\(x\in\mathcal E\)</span>, <span class="math inline">\(\partial P_f(x)\)</span> has the following properties,</p>
<ol type="1">
<li>Any <span class="math inline">\(V\in\partial P_f(x)\)</span> is self-adjoint.</li>
<li><span class="math inline">\(\langle Vd,d\rangle \ge ||Vd||^2\)</span> for any <span class="math inline">\(V\in\partial P_f(x)\)</span> and <span class="math inline">\(d\in\mathcal E\)</span>.</li>
</ol>
<h2 id="directional-derivatives-of-matrix-valued-functions">3.15 Directional derivatives of matrix-valued functions</h2>
<p>Suppose <span class="math inline">\(X\in\mathbb S^n\)</span> has spectral decomposition <span class="math inline">\(X=\bar P\Lambda \bar P^T\)</span> with its eigenvalues contained in an bounded open subset <span class="math inline">\(I\)</span> of <span class="math inline">\(\mathbb R\)</span>. Let <span class="math inline">\(f:I\rightarrow \mathbb R\)</span> be a scalar function. The Lowner spectral operator is defined by <span class="math display">\[
F(X)=\bar P\text{diag}(f(\lambda_1),\dots,f(\lambda_2))\bar P^T.
\]</span> <strong>Proposition</strong></p>
<p>The Lowner operator <span class="math inline">\(F(\cdot)\)</span> is differentiable at <span class="math inline">\(X\)</span> if and only if <span class="math inline">\(f(\cdot)\)</span> is differentiable at each <span class="math inline">\(\lambda_i (X)\)</span>. In this case, the Frchet derivative is given by <span class="math display">\[
F&#39;(X)[H]=\bar P[f^{[1]}(\Lambda)\circ (\bar P^TH\bar P)]\bar P^T\quad \forall H\in \mathbb S^n.
\]</span> Here given <span class="math inline">\(D=\text{diag}(d)\)</span>, <span class="math inline">\(f^{[1]}(D)\in \mathbb S^n\)</span> is defined by <span class="math display">\[
(f^{[1]}(D))_{ij}=
\begin{cases}
\frac{f(d_i)-f(d_j)}{d_i-d_j}&amp; d_i \neq d_j\\
f&#39;(d_i)&amp; d_i=d_j
\end{cases}
\]</span></p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Orange+Dragon</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zilongcheng@u.nus.edu" title="E-Mail &rarr; mailto:zilongcheng@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Orange+Dragon</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme  <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'k1NFV6E2jjtcuFpWbPUwvs04-MdYXbMMI',
    appKey: 'oCso3hdINWUXi0EtP7BsCUoY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
